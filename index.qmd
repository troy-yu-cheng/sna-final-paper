---
title: "Social Network Analysis"
subtitle: "Temporal Patterns of Misinformation Diffusion: A Multi-Stage Network Analysis of COVID-19 Origin Narratives on Twitter"
author: 
  - name: "Troy (Yu) Cheng"
    email: yc1317@georgetown.edu
    affiliation: Georgetown University
    corresponding: true
df-print: kable
title-block-banner: "#0a0e1a"
title-block-banner-color: "#4DB8FF"
execute:
  warning: false
date: 2025-04-28
date-modified: last-modified
format:
  html:
    embed-resources: true
    toc: true                 
    toc-title: "Contents"     
    toc-location: right       
    number-sections: true
    number-depth: 3       
    smooth-scroll: true       
    css: trycstyle.css 
    code-overflow: wrap
include-in-header:
  text: |
    <link rel="shortcut icon" href="assets/gu.ico" type="image/x-icon">           
highlight-style: nord       
engine: knitr
---
# Abstract

This study examines the temporal diffusion patterns of the misinformation narrative claiming that [COVID-19 was deliberately created/manufactured by China or the Wuhan laboratory](https://en.wikipedia.org/wiki/COVID-19_lab_leak_theory). The central research question asks: How did this narrative propagate across Twitter retweeter networks over time, and did early spreaders maintain structural advantages? Drawing on the Diffusion of Innovations Theory and Echo Chamber Theory, the study explores whether ideological homophily emerged as the narrative spread.

Using data from the [MuMiN](https://mumin-dataset.github.io/) project, six prominent misinformation claims between March 2020 and August 2021 were analyzed. Retweeters are treated as nodes, with ties defined by co-retweeting, co-replying, co-quoting, and following behaviors. The hypothesis posits that retweeters involved in earlier misinformation narratives occupy more central positions and retain higher connectivity over time.

Social Network Analysis (SNA) is employed to construct adjacency matrices, visualize evolving network structures, and compute centrality and modularity metrics. While temporal network growth is discontinuous across misinformation events, this study explores whether Temporal Exponential Random Graph Models (tERGM) can model these episodic formations by treating thematic similarity as a historical tie.

This research advances understanding of how misinformation narratives persist in online environments and how early participants shape long-term information diffusion.

# Introduction

# Literature Review


# Data Collection & Graph

## Dataset

This study uses secondary data from the **MuMiN Project** (Multi-platform Misinformation Influence Network), publicly available at: <https://data.bristol.ac.uk/data/en_GB/dataset/23yv276we2mll25fjakkfim2ml/resource/e63a98c4-b4d6-4a89-9be9-06875b83ce33?inner_span=True>

The dataset includes information about social media users, tweets, misinformation claims, and their relational behaviors across various platforms.

## Data Preparation

### Loading Required Packages

```{r}
#| label: set up packages
library(tidyverse)
library(readr)
library(statnet)
library(intergraph)
library(reshape2)
```

### Loading and Merging Data

```{r}
# Load data
tweets <- read_csv("data/mumin_csv/tweet.csv")
claims <- read_csv("data/mumin_csv/claim.csv")
tweet_claim <- read_csv("data/mumin_csv/tweet_discusses_claim.csv")
user_retweet <- read_csv("data/mumin_csv/user_retweeted_tweet.csv")

# Join tweet & claim via tweet_claim
tweet_claim_joined <- tweet_claim |> 
  rename(tweet_id = src, claim_id = tgt) |> 
  inner_join(claims, by = c("claim_id" = "id"))

# Join retweeters within users & claim via retweeted tweet
user_claim <- user_retweet |> 
  rename(retweeter_id = src, tweet_id = tgt) |> 
  inner_join(tweet_claim_joined, by = "tweet_id")

# Create a mapping of retweeters and tweets
retweet_map <- user_claim |> 
  select(retweeter_id, tweet_id) |> 
  distinct() 

# Basic data overview
table(user_claim$label)
head(table(user_claim$cluster_keywords))
```

### Filtering for Target Misinformation Narratives

This study focuses specifically on misinformation narratives claiming that **COVID-19 was deliberately created or spread by China or the Wuhan laboratory**. The following filtering steps were applied:

1. Filter keywords containing "china" or "wuhan".

2. Manually select misinformation topics directly related to the COVID-19 origin narrative.

3. Restrict the analysis to English-language posts labeled as "misinformation".

**Note: **

```{r}
user_claim_filtered <- user_claim |> 
  filter(str_detect(keywords, "china|wuhan")) 

head(unique(user_claim_filtered$keywords))

# Final filtered dataset
covid_wuhan_misinfo <- user_claim_filtered |>
  filter(keywords %in% c(
    "coronavirus comes biological laboratory wuhan",
    "including corona virus manufactured wuhan",
    "china created spread coronavirus",
    "announced coronavirus artificially created china",
    "china created coronavirus",
    "coronavirus created laboratory china",
    "honjo said china manufactured coronavirus",
    "said new coronavirus manufactured wuhan",
    "coronavirus tested biological weapon china",
    "china testing coronavirus biological weapon",
    "covid 19 lab china",
    "coronavirus natural worked wuhan laboratory",
    "coronavirus manufactured laboratory wuhan specifically",
    "doctor claimed coronavirus china create",
    "corona virus manufactured china",
    "says current coronavirus manufactured wuhan",
    "wuhan coronavirus created patented",
    "covid 19 disease manufactured china",
    "pandemic caused artificial virus china",
    "collaborator assures coronavirus manufactured wuhan",
    "concluded covid leaked wuhan lab",
    "coronavirus created united states china",
    "air comes china import coronavirus",
    "coronavirus released china"
  )) |>
  filter(language == "en") |>
  filter(label == "misinformation")

# Distribution of final misinformation topics
table(covid_wuhan_misinfo$keywords)
table(covid_wuhan_misinfo$cluster_keywords)
```

## Social Network Construction

This network models the co-dissemination behavior of misinformation retweeters. While the exact paths of information transmission are unknown, this co-spreader network captures the structural relationships that may facilitate misinformation diffusion.

### Co-Retweet Network

The `co-retweet` network represents precise co-dissemination behavior. A tie is created between two users only if they both retweeted the same specific misinformation tweet, which is identified by a shared `tweet_id`. Each edge is weighted by the number of such co-retweeting events, reflecting how often the same pair of users jointly retweeted the same pieces of misinformation. Though this relationship does not suggest that the paired users share the same opinion, it captures direct and synchronous amplification of specific misinformation artifacts. Therefore, by analyzing the weighted connections among pairs of retweeters, this network reveals which users are central to the misinformation dissemination process and whether specific communities form around the repeated propagation of misinformation. Such patterns are indicative of potential echo chambers and coordinated amplification efforts, both of which are crucial for understanding how misinformation sustains its visibility and influence over time.

```{r}
# Extract misinformation-related tweets id
misinfo_tweet_id <- unique(covid_wuhan_misinfo$tweet_id)

# Identify pairs of users who co-retweeted the same misinformation tweet
co_retweet_edges <- user_retweet |> 
  filter(tgt %in% misinfo_tweet_id) |> 
  rename(tweet_id = tgt, retweeter_id = src) |> 
  group_by(tweet_id) |> 
  summarise(
    pairs = list(as.data.frame(t(combn(retweeter_id, 2)))), 
    .groups = "drop"
  ) |> 
  unnest(pairs) |> 
  mutate(
    user_1 = pmin(V1, V2), # Ensure lower ID comes first
    user_2 = pmax(V1, V2) # Ensure higher ID comes second
  ) |> 
  select(user_1, user_2) |> 
  group_by(user_1, user_2) |> 
  summarise(weight = n(), .groups = "drop") |> 
  mutate(type = "co-retweet") |>
  filter(user_1 != user_2) |>
  mutate(
    user_1 = as.character(user_1),
    user_2 = as.character(user_2)
  )

# Obtain the full list of unique users involved
all_users <- unique(c(
  covid_wuhan_misinfo$retweeter_id,  
  co_retweet_edges$user_1, 
  co_retweet_edges$user_2
)) %>% 
  as.character() %>% 
  sort()

# Create initial matrix
adj_matrix_co_retweet <- acast(
  co_retweet_edges, 
  user_1 ~ user_2, 
  value.var = "weight", 
  fill = 0
)

# Ensure matrix completeness by adding missing rows and columns
missing_rows <- setdiff(all_users, rownames(adj_matrix_co_retweet))
missing_cols <- setdiff(all_users, colnames(adj_matrix_co_retweet))

if (length(missing_rows) > 0) {
  adj_matrix_co_retweet <- rbind(
    adj_matrix_co_retweet, 
    matrix(0, nrow = length(missing_rows), ncol = ncol(adj_matrix_co_retweet), 
           dimnames = list(missing_rows, colnames(adj_matrix_co_retweet)))
  )
}

if (length(missing_cols) > 0) {
  adj_matrix_co_retweet <- cbind(
    adj_matrix_co_retweet, 
    matrix(0, nrow = nrow(adj_matrix_co_retweet), ncol = length(missing_cols), 
           dimnames = list(rownames(adj_matrix_co_retweet), missing_cols))
  )
}

# Reorder the matrix to align rows and columns
adj_matrix_co_retweet <- adj_matrix_co_retweet[all_users, all_users]

# Export the adjacency matrix for further analysis
write.csv(adj_matrix_co_retweet, "data/adj_matrix_co_retweet.csv", row.names = TRUE)

# Check dimensions of the final matrix
ncol(adj_matrix_co_retweet)
nrow(adj_matrix_co_retweet)
```
### Co-quote

The `Co-quote` adjacency matrix captures the relationship between users who co-engaged with misinformation through the quoting function on Twitter. Specifically, two users are connected if one quoted a misinformation-related tweet and the other either quoted the same tweet or was the author of that quoted tweet. This interaction reflects a more deliberate and explicit form of engagement with misinformation content, often involving commentary or amplification.

The matrix is weighted based on the number of times each user pair engaged in co-quoting behavior. Diagonal elements (self-loops) are removed, and the matrix is fully completed to ensure that all users appear in both rows and columns, enabling comprehensive analysis of the co-quote network structure.


我们能确定关系的是retweeter of quoting tweet和retweeter of quoted tweet之间的关系

Co-Quote：
A retweeted 了某个 quoting tweet（引用了一条 misinformation tweet 的 tweet）。

B retweeted 了被引用的那条 misinformation tweet。

→ 连 A 和 B。

现实意义：

A 通过 retweet 了一个引用谣言的内容，参与了围绕谣言的传播和讨论。

B 直接 retweet 了这个谣言。

他们都围绕着同一 misinformation 内容参与了传播，但路径不同。


```{r}
# Co-Quote Edges
co_quote_edges <- read_csv("data/mumin_csv/reply_quote_of_tweet.csv") |>
  filter(tgt %in% misinfo_tweet_id | src %in% misinfo_tweet_id) |> 
  rename(
    quoted_tweet_id = tgt,    # 被引用的 tweet
    quoting_tweet_id = src    # 发起引用的 tweet
  ) |> 
  # 找 quoting tweet 是谁 retweeted 的（即 quoting user）
  left_join(retweet_map, by = c("quoting_tweet_id" = "tweet_id")) |> 
  rename(quoting_user = retweeter_id) |> 
  # 找 quoted tweet 是谁 retweeted 的（即 quoted user）
  left_join(retweet_map, by = c("quoted_tweet_id" = "tweet_id")) |> 
  rename(quoted_user = retweeter_id) |> 
  filter(!is.na(quoting_user) & !is.na(quoted_user)) |> 
  mutate(
    user_1 = pmin(quoting_user, quoted_user),
    user_2 = pmax(quoting_user, quoted_user)
  ) |> 
  select(user_1, user_2) |> 
  group_by(user_1, user_2) |> 
  summarise(weight = n(), .groups = "drop") |> 
  mutate(type = "co-quote") |> 
  mutate(type = "co-quote") |>
  filter(user_1 != user_2) |>
  mutate(
    user_1 = as.character(user_1),
    user_2 = as.character(user_2)
  )

# Step 1: 获取完整 retweeter 列表（包含有无 co-quote 行为的）
all_retweeters <- unique(c(
  covid_wuhan_misinfo$retweeter_id,  # 核心传播者
  co_quote_edges$user_1, 
  co_quote_edges$user_2
)) %>% 
  as.character() %>% 
  sort()

# Step 2: 创建完整矩阵
adj_matrix_co_quote <- acast(
  co_quote_edges, 
  user_1 ~ user_2, 
  value.var = "weight", 
  fill = 0
)

# Step 3: 添加缺失行和列
missing_rows <- setdiff(all_retweeters, rownames(adj_matrix_co_quote))
missing_cols <- setdiff(all_retweeters, colnames(adj_matrix_co_quote))

if (length(missing_rows) > 0) {
  adj_matrix_co_quote <- rbind(
    adj_matrix_co_quote, 
    matrix(0, nrow = length(missing_rows), ncol = ncol(adj_matrix_co_quote), 
           dimnames = list(missing_rows, colnames(adj_matrix_co_quote)))
  )
}

if (length(missing_cols) > 0) {
  adj_matrix_co_quote <- cbind(
    adj_matrix_co_quote, 
    matrix(0, nrow = nrow(adj_matrix_co_quote), ncol = length(missing_cols), 
           dimnames = list(rownames(adj_matrix_co_quote), missing_cols))
  )
}

# Step 4: 确保矩阵对齐
adj_matrix_co_quote <- adj_matrix_co_quote[all_retweeters, all_retweeters]

# 保存为 CSV
write.csv(adj_matrix_co_quote, "data/adj_matrix_co_quote.csv", row.names = TRUE)


nrow(adj_matrix_co_quote)
ncol(adj_matrix_co_quote)


```

### Co-reply

我们能确定关系的是retweeter of replying tweet和retweeter of replyed tweet之间的关系

A retweeted 了一个 replying tweet（对一条 misinformation tweet 进行了 reply 的 tweet）。

B retweeted 了被 reply 的那条 misinformation tweet。

→ 连 A 和 B。

现实意义：

A 通过 retweet 了一个 reply 帖子，参与了围绕 misinformation 的讨论链。

B 是直接的传播者。

他们是围绕相同 misinformation 内容的 间接传播参与者。

#### co_reply_edges Explanation (General Co-Reply Network)
This adjacency matrix captures the co-reply relationship between users who have engaged with each other via the reply function on Twitter in the context of misinformation.
In this network, two users are connected if:

One replied to a tweet, and

The other either replied to the same tweet or authored the replied tweet.

However, due to limitations in the dataset (missing tweet author information), this matrix primarily reflects shared engagement with the same tweet rather than direct author-replier relationships.

Note: In this case, no edges were detected, possibly because the retweeters of misinformation tweets did not participate significantly in reply activities.

```{r}
# Co-reply
co_reply_edges <- read_csv("data/mumin_csv/reply_reply_to_tweet.csv") |>
  filter(tgt %in% misinfo_tweet_id | src %in% misinfo_tweet_id) |> 
  rename(
    replied_tweet_id = tgt, 
    replying_tweet_id = src
  ) |> 
  # 找出两条 tweet 各自的 retweeter
  left_join(retweet_map, by = c("replied_tweet_id" = "tweet_id")) |> 
  rename(user_replied = retweeter_id) |> 
  left_join(retweet_map, by = c("replying_tweet_id" = "tweet_id")) |> 
  rename(user_replying = retweeter_id) |> 
  filter(!is.na(user_replied) & !is.na(user_replying)) |> 
  mutate(
    user_1 = pmin(user_replied, user_replying),
    user_2 = pmax(user_replied, user_replying)
  ) |> 
  select(user_1, user_2) |> 
  group_by(user_1, user_2) |> 
  summarise(weight = n(), .groups = "drop") |> 
  mutate(type = "co-reply")

table(co_reply_edges$weight)

reply_data <- read_csv("data/mumin_csv/reply_reply_to_tweet.csv")
sum(reply_data$src %in% retweet_map$tweet_id)  # 有多少 replying tweets 被 retweeted？
sum(reply_data$tgt %in% retweet_map$tweet_id)  # 有多少 replied tweets 被 retweeted？


```

#### co_reply_edges_v2 Explanation (Focused Co-Reply on Misinformation Tweets)

This adjacency matrix focuses exclusively on replies to misinformation tweets.
It captures the relationship between users who retweeted the same misinformation tweet and also replied to it.

In this network, two users are connected if they both retweeted a misinformation tweet and subsequently replied to that tweet or engaged in discussions around it.

This captures shared attention and engagement specifically around misinformation content, providing insight into how users might form conversational clusters around these topics.

```{r}
co_reply_edges_v2 <- read_csv("data/mumin_csv/reply_reply_to_tweet.csv") |>
  filter(tgt %in% misinfo_tweet_id) |>  # 被 reply 的 tweet 是 misinformation
  rename(replied_tweet_id = tgt) |> 
  # 找出这个 misinformation tweet 被谁 retweeted 过
  left_join(retweet_map, by = c("replied_tweet_id" = "tweet_id")) |>
  rename(user_replied = retweeter_id) |> 
  filter(!is.na(user_replied)) |> 
  group_by(replied_tweet_id) |> 
  summarise(
    pairs = list(as.data.frame(t(combn(user_replied, 2)))), 
    .groups = "drop"
  ) |> 
  unnest(pairs) |> 
  mutate(
    user_1 = pmin(V1, V2),
    user_2 = pmax(V1, V2)
  ) |> 
  select(user_1, user_2) |> 
  group_by(user_1, user_2) |> 
  summarise(weight = n(), .groups = "drop") |> 
  mutate(type = "co-reply-v2")  |>
  filter(user_1 != user_2) |>
  mutate(
    user_1 = as.character(user_1),
    user_2 = as.character(user_2)
  )

# Step 1: 定义所有 retweeters（包括参与和未参与 reply 的）
all_retweeters_reply <- unique(c(
  covid_wuhan_misinfo$retweeter_id,  # 核心传播者
  co_reply_edges_v2$user_1, 
  co_reply_edges_v2$user_2
)) %>% 
  as.character() %>% 
  sort()

# Step 2: 创建 Adjacency Matrix
adj_matrix_co_reply_v2 <- acast(
  co_reply_edges_v2, 
  user_1 ~ user_2, 
  value.var = "weight", 
  fill = 0
)

# Step 3: 确保矩阵包含所有 retweeters
missing_rows <- setdiff(all_retweeters_reply, rownames(adj_matrix_co_reply_v2))
missing_cols <- setdiff(all_retweeters_reply, colnames(adj_matrix_co_reply_v2))

if (length(missing_rows) > 0) {
  adj_matrix_co_reply_v2 <- rbind(
    adj_matrix_co_reply_v2, 
    matrix(0, nrow = length(missing_rows), ncol = ncol(adj_matrix_co_reply_v2), 
           dimnames = list(missing_rows, colnames(adj_matrix_co_reply_v2)))
  )
}

if (length(missing_cols) > 0) {
  adj_matrix_co_reply_v2 <- cbind(
    adj_matrix_co_reply_v2, 
    matrix(0, nrow = nrow(adj_matrix_co_reply_v2), ncol = length(missing_cols), 
           dimnames = list(rownames(adj_matrix_co_reply_v2), missing_cols))
  )
}

# Step 4: Reorder to ensure rows and columns match
adj_matrix_co_reply_v2 <- adj_matrix_co_reply_v2[all_retweeters_reply, all_retweeters_reply]

# Step 5: 保存 CSV
write.csv(adj_matrix_co_reply_v2, "data/adj_matrix_co_reply_v2.csv", row.names = TRUE)

nrow(adj_matrix_co_reply_v2)
ncol(adj_matrix_co_reply_v2)


```

This step ensures that the adjacency matrix fully represents the social structure by including all users who participated in the retweet network, regardless of their direct involvement in reply behaviors.

Users who did not engage in replying are represented as isolated nodes (zero rows and columns), preserving the completeness of the network for structural analysis.


### Follow

#### Undirected follow

This edge represents an undirected tie between any two users who are both retweeters of misinformation tweets and have a recorded follower-followee relationship in either direction. However, the relationship is not necessarily mutual. It simply captures the existence of a connection in the follower network among the misinformation retweeters, regardless of direction.

src（user_a）是follower，tgt（user_b）是followee。

filter(user_a %in% retweeter_id_misinfo & user_b %in% retweeter_id_misinfo)：
→ 表示这条边仅要求双方都是 retweeters，不要求他们互相关注。

pmin/pmax 是为了去掉方向性，把边转成无向的。

这不是 mutual follow（互相关注），而是只要 follower 和 followee 都是 retweeter，就创建一条无向边。

```{r}
# Follow
# 获取 retweeter 列表
retweeter_id_misinfo <- unique(covid_wuhan_misinfo$retweeter_id)

# 构造 Follow 边
retweeter_follow_edges <- read_csv("data/mumin_csv/user_follows_user.csv") |>
  rename(user_a = src, user_b = tgt) |> 
  # 保留两者都在 retweeter 列表中的记录
  filter(user_a %in% retweeter_id_misinfo & user_b %in% retweeter_id_misinfo) |> 
  mutate(
    user_1 = pmin(user_a, user_b),  # 保持小的 ID 在前，去掉方向性
    user_2 = pmax(user_a, user_b)
  ) |> 
  select(user_1, user_2) |> 
  distinct() |>  # 去重，防止重复记录 A -> B 和 B -> A
  mutate(weight = 1, type = "follow")

retweeter_follow_edges

```

all 0 matrix:

```{r}
# 获取所有 retweeters
all_retweeters <- sort(unique(c(
  covid_wuhan_misinfo$retweeter_id, 
  retweeter_follow_edges$user_1, 
  retweeter_follow_edges$user_2
))) %>% as.character()

# 创建全 0 矩阵
adj_matrix_follow <- matrix(
  0, 
  nrow = length(all_retweeters), 
  ncol = length(all_retweeters),
  dimnames = list(all_retweeters, all_retweeters)
)

# 转成数据框导出
write.csv(adj_matrix_follow, "data/adj_matrix_follow.csv", row.names = TRUE)

```

#### Directed follow

This edge preserves the directionality of follower relationships between misinformation retweeters (e.g., User A follows User B). It allows for modeling influence flow and assessing asymmetric structures (e.g., hubs or hierarchies) in the spread of misinformation.

follower → followee 表示单向的关注。

同样地，只要两者都在 retweeter 列表中，就保留这个方向性关系。

这是真正的 directed edge，表示“谁关注了谁”，方向性保留。

```{r}
retweeter_follow_edges_directed <- read_csv("data/mumin_csv/user_follows_user.csv") |> 
  rename(follower = src, followee = tgt) |> 
  # 只保留 retweeter 之间的关注关系（有方向）
  filter(follower %in% retweeter_id_misinfo & followee %in% retweeter_id_misinfo) |> 
  mutate(
    weight = 1,          # 默认权重为 1
    type = "follow_directed"      # 明确说明是关注关系
  ) |> 
  select(follower, followee, weight, type)

```


```{r}
follow_data<-read_csv("data/mumin_csv/user_follows_user.csv") 
sum(follow_data$src %in% covid_wuhan_misinfo$retweeter_id)
sum(follow_data$tgt %in% covid_wuhan_misinfo$retweeter_id)

sum(covid_wuhan_misinfo$retweeter_id%in% follow_data$src)
sum(covid_wuhan_misinfo$retweeter_id%in% follow_data$tgt)

covid_wuhan_misinfo |>
  group_by(date, keywords) |>
  select(date, keywords) |>
  arrange(date)|>
  unique()

```



# Analysis

## Co-retweet Net

```{r}
# 读取 co-retweet adjacency matrix
adj_matrix_co_retweet <- read.csv(
  "data/adj_matrix_co_retweet.csv", 
  row.names = 1, 
  check.names = FALSE  # 关键参数，禁止自动加 x 前缀
)

# 构建 network 对象（无向图）
net_co_retweet <- network(as.matrix(adj_matrix_co_retweet), directed = FALSE, matrix.type = "adjacency")

get.vertex.attribute(net_co_retweet, "vertex.names")

# 获取分组信息
retweeter_group <- covid_wuhan_misinfo %>%
  mutate(retweeter_id = as.character(retweeter_id)) %>%
  select(retweeter_id, date, keywords) %>%
  distinct() %>%
  arrange(date) %>%
  mutate(group = as.numeric(factor(paste(date, keywords), levels = unique(paste(date, keywords)))))

# 将 Group 信息添加到节点属性中
set.vertex.attribute(
  net_co_retweet, 
  "group", 
  retweeter_group$group[match(get.vertex.attribute(net_co_retweet, "vertex.names"), retweeter_group$retweeter_id)]
)

get.vertex.attribute(net_co_retweet, "group")

for (g in 1:6) {
  sub_g <- get.inducedSubgraph(net_co_retweet, 
                               which(get.vertex.attribute(net_co_retweet, "group") == g))
  plot.network(sub_g, 
                displaylabels = FALSE, 
                vertex.cex = 1, 
                vertex.col = "skyblue", 
                main = paste("Subgraph for Group", g))
}


```

```{r}
# 1. 获取阶段信息
group_stages <- sort(unique(get.vertex.attribute(net_co_retweet, "group")))

results <- data.frame()  # 清空或初始化

for (g in group_stages) {
  sub_nodes <- which(get.vertex.attribute(net_co_retweet, "group") <= g)
  sub_g <- get.inducedSubgraph(net_co_retweet, v = sub_nodes)
  
  adj_mat <- as.matrix.network(sub_g, matrix.type = "adjacency")
  
  results <- rbind(results, data.frame(
    Stage = g,
    Nodes = network.size(sub_g),
    Edges = network.edgecount(sub_g),
    Density = gden(sub_g),
    Degree_Centralization = centralization(adj_mat, FUN = degree, mode = "graph"),
    Closeness_Centralization = centralization(adj_mat, FUN = closeness, mode = "graph"),
    Betweenness_Centralization = centralization(adj_mat, FUN = betweenness, mode = "graph"),
    Transitivity = gtrans(sub_g, mode = "graph")
  ))
}
# 4. 查看结果
print(results)
```


1️⃣ 密度 (Density) 下降清晰可见
第一阶段密度 = 1.0，完全图（每个节点都互相关联）。

随着节点增加，密度迅速下降到 0.13 左右。

📌 解释：

最初的核心传播者高度互动，形成了小规模的“精英传播圈”（如密闭小团体）。

随着传播扩散，边缘用户（外围 retweeters）被逐渐吸引进来，但他们之间并没有大量的相互联系，仅与核心传播者或少数用户连接，导致网络越来越稀疏。

2️⃣ Degree Centralization 稍微上升，但始终偏低
从 0.048 稳定上升到 0.086。

表明：

虽然有部分“超级传播者”节点出现（拥有更多连接），

但整个网络并没有形成强烈的“单一核心控制”现象，还是较为分散的。

📌 对比分析：

这符合谣言传播中的“去中心化特征”：

虽然有影响力大的用户，但也有大量中小节点参与传播，形成了“分布式扩散”。

3️⃣ Closeness 和 Betweenness Centralization 一直为 0
📌 解释：

这意味着：

没有节点真正承担“中介桥梁”的角色（betweenness = 0）。

没有节点在信息传递路径中占据显著位置（closeness = 0）。

很可能是因为这个网络是 无向图且小团体内部强连接，导致大部分节点路径长度相似，或根本不需要通过中介节点就能直接连接。

📌 洞察：

这是典型的“群团式传播”，不同群体内部连通紧密，但群体之间的桥梁节点稀缺。

符合 Echo Chamber Theory 的描述 —— 信息在群体内部反复传播，很难跨群体传递。

4️⃣ Transitivity (全局聚类系数) 极高
一直维持在 接近 1.0，哪怕节点数量增加也只有极微弱下降。

📌 解释：

网络中绝大部分三元闭环都存在 —— 换句话说，“朋友的朋友也是朋友”。

再次印证了高度群体化、小圈子强连接的传播模式。

📊 综合洞察总结
初期核心小团体主导，随着传播演化，网络规模扩大但联系稀疏。

网络保持高度的聚类和局部性，群体之间的传播壁垒强烈。

信息的跨群体扩散能力较弱，符合 回音室效应（Echo Chamber） 和 去中心化的分散传播特征。

没有形成典型的“明星用户主导的中心化扩散”，而是多群体、小圈子在独立传播。


```{r}
results <- data.frame()  # 清空或初始化

for (g in group_stages) {
  sub_nodes <- which(get.vertex.attribute(net_co_retweet, "group") == g)
  sub_g <- get.inducedSubgraph(net_co_retweet, v = sub_nodes)
  
  adj_mat <- as.matrix.network(sub_g, matrix.type = "adjacency")
  
  results <- rbind(results, data.frame(
    Stage = g,
    Nodes = network.size(sub_g),
    Edges = network.edgecount(sub_g),
    Density = gden(sub_g),
    Degree_Centralization = centralization(adj_mat, FUN = degree, mode = "graph"),
    Closeness_Centralization = centralization(adj_mat, FUN = closeness, mode = "graph"),
    Betweenness_Centralization = centralization(adj_mat, FUN = betweenness, mode = "graph"),
    Transitivity = gtrans(sub_g, mode = "graph")
  ))
}
# 4. 查看结果
print(results)


```


1️⃣ 初始阶段 (Stage 1)
完美的 完全图 (Density=1)，节点之间全连接。

各种中心性为 0 → 权力极度均衡，没有明显的核心节点。

Transitivity = 1 → 完美的三角闭环，这通常发生在小团体、同质性极高的圈子。

2️⃣ 传播爆发 (Stage 2)
Density 降到 0.51，但网络规模扩大，Edges 增多。

Degree Centralization 开始上升，表明出现了关键传播者（核心节点逐渐形成）。

依然是高闭包（Transitivity=1），说明社群内部联系紧密，但开始显现传播分工。

3️⃣ 短暂收缩 (Stage 3)
节点数和边数大幅减少，网络再次变成完全图。

可能是信息传播的 短暂停滞 或 小圈层独立传播。

4️⃣ 扩展尝试 (Stage 4)
节点数再次上升，但 Density 很低（0.24），网络松散。

说明信息扩散到了更广泛的受众，但彼此之间联系不强。

中心性仍低，未形成稳定的信息传播主导者。

5️⃣ 核心扩散 (Stage 5)
虽然节点数不多，但出现了显著的中心性特征：

Degree Centralization 高达 0.36

Closeness Centralization 达到 0.47

Betweenness Centralization 也有 0.16

这一阶段可能出现了关键意见领袖或超级传播者，对信息扩散起到显著作用。

Transitivity 降低到 0.85 → 网络内闭环减少，更多 桥接型节点 出现。

6️⃣ 最末阶段 (Stage 6)
网络再次收缩成完全图，群体非常小，但联系非常紧密。

可能是核心群体的余波交流，但未再继续向外传播。

## tERGM sample

```{r}

# # Step 1: 准备网络列表（每个时间点一个 network）
# network_list <- list()
# for (g in group_stages) {
#   sub_nodes <- which(get.vertex.attribute(net_co_retweet, "group") <= g)
#   sub_g <- get.inducedSubgraph(net_co_retweet, v = sub_nodes)
#   network_list[[as.character(g)]] <- sub_g
# }
# 
# # Step 2: 构建模型
# # 使用简单结构性特征作为例子
# model <- tergm(
#   network_list ~ Form(~edges + gwesp(0.5, fixed = TRUE)) + 
#     Dissolution(~edges), 
#   estimate = "CMLE"
# )
# 
# summary(model)

```


# Reference


# Appendix

Raw dataset download link: <https://data.bristol.ac.uk/data/en_GB/dataset/23yv276we2mll25fjakkfim2ml/resource/e63a98c4-b4d6-4a89-9be9-06875b83ce33?inner_span=True>


After downloading all files, create a `data` folder in your working directory and place the files inside it.

Follow the setup instructions provided [here](https://colab.research.google.com/drive/1JCjgg3moGBOuZk4iVjBpQNqgsAYFyNoS#scrollTo=YS1uxj-59UmA). Use the shell to install required packages via `pip` as directed.

Next, open `VS Code`, and run the following Python script to convert `.pkl` files to `.csv`:

```{python}
#| eval: false
#| include: true

import pandas as pd
import os

input_dir = "data/mumin"
output_dir = "data/mumin_csv"
os.makedirs(output_dir, exist_ok=True)

file_names = [
    "claim",
    "user",
    "tweet",
    "reply",
    "article",
    "tweet_discusses_claim",
    "article_discusses_claim",
    "user_follows_user",
    "user_retweeted_tweet",
    "reply_quote_of_tweet",
    "reply_reply_to_tweet"
]

for name in file_names:
    input_path = os.path.join(input_dir, name)
    output_path = os.path.join(output_dir, f"{name}.csv")
    try:
        df = pd.read_pickle(input_path, compression="xz")
        df.to_csv(output_path, index=False)
        print(f"Saved: {output_path}")
    except Exception as e:
        print(f"Failed to convert {name}: {e}")

```

This script will generate the corresponding CSV files for further analysis.

Then, preview the Data in R:

```{r}
#| eval: false
#| include: true
# Set the data path 
data_path <- "data/mumin_csv/"

# Define the list of files to read
files <- c(
  "claim.csv", "user.csv", "tweet.csv", "reply.csv", "article.csv",
  "tweet_discusses_claim.csv", "article_discusses_claim.csv",
  "user_follows_user.csv", "user_retweeted_tweet.csv",
  "reply_quote_of_tweet.csv", "reply_reply_to_tweet.csv"
)

# Preview the first few rows of each file 
previews <- lapply(files, function(fname) {
  fpath <- file.path(data_path, fname)
  tryCatch(
    read_csv(fpath, n_max = 5),
    error = function(e) tibble::tibble(error = paste("Error:", e$message))
  )
})

names(previews) <- files

```

```{=html}
<script>
document.addEventListener("DOMContentLoaded", function() {
    const toc = document.getElementById("TOC");
    if (toc) {
        const sourceLink = document.createElement("div");
        sourceLink.innerHTML = `
            <div class="toc-source">
                <a href="https://github.com/troy-yu-cheng/sna-final-paper" 
                   target="_blank" 
                   class="github-button">
                   <svg xmlns="http://www.w3.org/2000/svg" 
                        viewBox="0 0 24 24" 
                        width="16" 
                        height="16" 
                        fill="currentColor"
                        style="vertical-align: middle; margin-right: 5px;">
                     <path d="M12 0C5.373 0 0 5.373 0 12c0 5.303 3.438 9.8 8.207 11.387.6.113.82-.26.82-.577v-2.157c-3.338.726-4.033-1.416-4.033-1.416-.546-1.386-1.332-1.756-1.332-1.756-1.09-.745.083-.73.083-.73 1.205.084 1.84 1.237 1.84 1.237 1.07 1.832 2.807 1.303 3.492.996.108-.774.418-1.303.76-1.602-2.665-.3-5.466-1.332-5.466-5.93 0-1.311.468-2.382 1.237-3.222-.124-.302-.536-1.52.118-3.163 0 0 1.008-.322 3.3 1.23a11.516 11.516 0 0 1 3.002-.403 11.486 11.486 0 0 1 3.002.403c2.292-1.552 3.3-1.23 3.3-1.23.654 1.644.242 2.861.118 3.163.77.84 1.236 1.911 1.236 3.222 0 4.61-2.807 5.627-5.48 5.922.43.372.812 1.103.812 2.222v3.293c0 .321.218.694.825.576C20.565 21.796 24 17.3 24 12 24 5.373 18.627 0 12 0z"/>
                   </svg>
                   View source
                </a>
            </div>
        `;
        toc.appendChild(sourceLink);
    }
});
</script>
```

