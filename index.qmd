---
title: "Social Network Analysis"
subtitle: "Temporal Patterns of Misinformation Diffusion: A Multi-Stage Network Analysis of COVID-19 Origin Narratives on Twitter"
author: 
  - name: "Troy (Yu) Cheng"
    email: yc1317@georgetown.edu
    affiliation: Georgetown University
    corresponding: true
df-print: kable
title-block-banner: "#0a0e1a"
title-block-banner-color: "#4DB8FF"
execute:
  warning: false
date: 2025-04-28
date-modified: last-modified
format:
  html:
    embed-resources: true
    toc: true                 
    toc-title: "Contents"     
    toc-location: right       
    number-sections: true
    number-depth: 3       
    smooth-scroll: true       
    css: trycstyle.css 
    code-overflow: wrap
include-in-header:
  text: |
    <link rel="shortcut icon" href="assets/gu.ico" type="image/x-icon">           
highlight-style: nord       
engine: knitr
---
# Abstract

This study examines the temporal diffusion patterns of the misinformation narrative claiming that [COVID-19 was deliberately created/manufactured by China or the Wuhan laboratory](https://en.wikipedia.org/wiki/COVID-19_lab_leak_theory). The central research question asks: How did this narrative propagate across Twitter retweeter networks over time, and did early spreaders maintain structural advantages? Drawing on the Diffusion of Innovations Theory and Echo Chamber Theory, the study explores whether ideological homophily emerged as the narrative spread.

Using data from the [MuMiN](https://mumin-dataset.github.io/) project, six prominent misinformation claims between March 2020 and August 2021 were analyzed. Retweeters are treated as nodes, with ties defined by co-retweeting, co-replying, co-quoting, and following behaviors. The hypothesis posits that retweeters involved in earlier misinformation narratives occupy more central positions and retain higher connectivity over time.

Social Network Analysis (SNA) is employed to construct adjacency matrices, visualize evolving network structures, and compute centrality and modularity metrics. While temporal network growth is discontinuous across misinformation events, this study explores whether Temporal Exponential Random Graph Models (tERGM) can model these episodic formations by treating thematic similarity as a historical tie.

This research advances understanding of how misinformation narratives persist in online environments and how early participants shape long-term information diffusion.

# Introduction

# Literature Review


# Data Collection & Graph

## Dataset

This study uses secondary data from the **MuMiN Project** (Multi-platform Misinformation Influence Network), publicly available at: <https://data.bristol.ac.uk/data/en_GB/dataset/23yv276we2mll25fjakkfim2ml/resource/e63a98c4-b4d6-4a89-9be9-06875b83ce33?inner_span=True>

The dataset includes information about social media users, tweets, misinformation claims, and their relational behaviors across various platforms.

## Data Preparation

### Loading Required Packages

```{r}
#| label: set up packages
library(tidyverse)
library(readr)
library(statnet)
library(intergraph)
library(reshape2)
```

### Loading and Merging Data

```{r}
# Load data
tweets <- read_csv("data/mumin_csv/tweet.csv")
claims <- read_csv("data/mumin_csv/claim.csv")
tweet_claim <- read_csv("data/mumin_csv/tweet_discusses_claim.csv")
user_retweet <- read_csv("data/mumin_csv/user_retweeted_tweet.csv")

# Join tweet & claim via tweet_claim
tweet_claim_joined <- tweet_claim |> 
  rename(tweet_id = src, claim_id = tgt) |> 
  inner_join(claims, by = c("claim_id" = "id"))

# Join retweeters within users & claim via retweeted tweet
user_claim <- user_retweet |> 
  rename(retweeter_id = src, tweet_id = tgt) |> 
  inner_join(tweet_claim_joined, by = "tweet_id")

# Create a mapping of retweeters and tweets
retweet_map <- user_claim |> 
  select(retweeter_id, tweet_id) |> 
  distinct() 

# Basic data overview
table(user_claim$label)
head(table(user_claim$cluster_keywords))
```

### Filtering for Target Misinformation Narratives

This study focuses specifically on misinformation narratives claiming that **COVID-19 was deliberately created or spread by China or the Wuhan laboratory**. The following filtering steps were applied:

1. Filter keywords containing "china" or "wuhan".

2. Manually select misinformation topics directly related to the COVID-19 origin narrative.

3. Restrict the analysis to English-language posts labeled as "misinformation".

**Note: **

```{r}
user_claim_filtered <- user_claim |> 
  filter(str_detect(keywords, "china|wuhan")) 

head(unique(user_claim_filtered$keywords))

# Final filtered dataset
covid_wuhan_misinfo <- user_claim_filtered |>
  filter(keywords %in% c(
    "coronavirus comes biological laboratory wuhan",
    "including corona virus manufactured wuhan",
    "china created spread coronavirus",
    "announced coronavirus artificially created china",
    "china created coronavirus",
    "coronavirus created laboratory china",
    "honjo said china manufactured coronavirus",
    "said new coronavirus manufactured wuhan",
    "coronavirus tested biological weapon china",
    "china testing coronavirus biological weapon",
    "covid 19 lab china",
    "coronavirus natural worked wuhan laboratory",
    "coronavirus manufactured laboratory wuhan specifically",
    "doctor claimed coronavirus china create",
    "corona virus manufactured china",
    "says current coronavirus manufactured wuhan",
    "wuhan coronavirus created patented",
    "covid 19 disease manufactured china",
    "pandemic caused artificial virus china",
    "collaborator assures coronavirus manufactured wuhan",
    "concluded covid leaked wuhan lab",
    "coronavirus created united states china",
    "air comes china import coronavirus",
    "coronavirus released china"
  )) |>
  filter(language == "en") |>
  filter(label == "misinformation")

# Distribution of final misinformation topics
table(covid_wuhan_misinfo$keywords)
table(covid_wuhan_misinfo$cluster_keywords)
```

## Social Network Construction

This network models the co-dissemination behavior of misinformation retweeters. While the exact paths of information transmission are unknown, this co-spreader network captures the structural relationships that may facilitate misinformation diffusion.

### Co-Retweet Network

The `co-retweet` network represents precise co-dissemination behavior. A tie is created between two users only if they both retweeted the same specific misinformation tweet, which is identified by a shared `tweet_id`. Each edge is weighted by the number of such co-retweeting events, reflecting how often the same pair of users jointly retweeted the same pieces of misinformation. Though this relationship does not suggest that the paired users share the same opinion, it captures direct and synchronous amplification of specific misinformation artifacts. Therefore, by analyzing the weighted connections among pairs of retweeters, this network reveals which users are central to the misinformation dissemination process and whether specific communities form around the repeated propagation of misinformation. Such patterns are indicative of potential echo chambers and coordinated amplification efforts, both of which are crucial for understanding how misinformation sustains its visibility and influence over time.

```{r}
# Extract misinformation-related tweets id
misinfo_tweet_id <- unique(covid_wuhan_misinfo$tweet_id)

# Identify pairs of users who co-retweeted the same misinformation tweet
co_retweet_edges <- user_retweet |> 
  filter(tgt %in% misinfo_tweet_id) |> 
  rename(tweet_id = tgt, retweeter_id = src) |> 
  group_by(tweet_id) |> 
  summarise(
    pairs = list(as.data.frame(t(combn(retweeter_id, 2)))), 
    .groups = "drop"
  ) |> 
  unnest(pairs) |> 
  mutate(
    user_1 = pmin(V1, V2), # Ensure lower ID comes first
    user_2 = pmax(V1, V2) # Ensure higher ID comes second
  ) |> 
  select(user_1, user_2) |> 
  group_by(user_1, user_2) |> 
  summarise(weight = n(), .groups = "drop") |> 
  mutate(type = "co-retweet") |>
  filter(user_1 != user_2) |>
  mutate(
    user_1 = as.character(user_1),
    user_2 = as.character(user_2)
  )

# Obtain the full list of unique users involved
all_users <- unique(c(
  covid_wuhan_misinfo$retweeter_id,  
  co_retweet_edges$user_1, 
  co_retweet_edges$user_2
)) %>% 
  as.character() %>% 
  sort()

# Create initial matrix
adj_matrix_co_retweet <- acast(
  co_retweet_edges, 
  user_1 ~ user_2, 
  value.var = "weight", 
  fill = 0
)

# Ensure matrix completeness by adding missing rows and columns
missing_rows <- setdiff(all_users, rownames(adj_matrix_co_retweet))
missing_cols <- setdiff(all_users, colnames(adj_matrix_co_retweet))

if (length(missing_rows) > 0) {
  adj_matrix_co_retweet <- rbind(
    adj_matrix_co_retweet, 
    matrix(0, nrow = length(missing_rows), ncol = ncol(adj_matrix_co_retweet), 
           dimnames = list(missing_rows, colnames(adj_matrix_co_retweet)))
  )
}

if (length(missing_cols) > 0) {
  adj_matrix_co_retweet <- cbind(
    adj_matrix_co_retweet, 
    matrix(0, nrow = nrow(adj_matrix_co_retweet), ncol = length(missing_cols), 
           dimnames = list(rownames(adj_matrix_co_retweet), missing_cols))
  )
}

# Reorder the matrix to align rows and columns
adj_matrix_co_retweet <- adj_matrix_co_retweet[all_users, all_users]

# Export the adjacency matrix for further analysis
write.csv(adj_matrix_co_retweet, "data/adj_matrix_co_retweet.csv", row.names = TRUE)

# Check dimensions of the final matrix
ncol(adj_matrix_co_retweet)
nrow(adj_matrix_co_retweet)
```
### Co-quote

The `Co-quote` adjacency matrix captures the relationship between users who co-engaged with misinformation through the quoting function on Twitter. Specifically, two users are connected if one quoted a misinformation-related tweet and the other either quoted the same tweet or was the author of that quoted tweet. This interaction reflects a more deliberate and explicit form of engagement with misinformation content, often involving commentary or amplification.

The matrix is weighted based on the number of times each user pair engaged in co-quoting behavior. Diagonal elements (self-loops) are removed, and the matrix is fully completed to ensure that all users appear in both rows and columns, enabling comprehensive analysis of the co-quote network structure.


æˆ‘ä»¬èƒ½ç¡®å®šå…³ç³»çš„æ˜¯retweeter of quoting tweetå’Œretweeter of quoted tweetä¹‹é—´çš„å…³ç³»

Co-Quoteï¼š
A retweeted äº†æŸä¸ª quoting tweetï¼ˆå¼•ç”¨äº†ä¸€æ¡ misinformation tweet çš„ tweetï¼‰ã€‚

B retweeted äº†è¢«å¼•ç”¨çš„é‚£æ¡ misinformation tweetã€‚

â†’ è¿ A å’Œ Bã€‚

ç°å®æ„ä¹‰ï¼š

A é€šè¿‡ retweet äº†ä¸€ä¸ªå¼•ç”¨è°£è¨€çš„å†…å®¹ï¼Œå‚ä¸äº†å›´ç»•è°£è¨€çš„ä¼ æ’­å’Œè®¨è®ºã€‚

B ç›´æ¥ retweet äº†è¿™ä¸ªè°£è¨€ã€‚

ä»–ä»¬éƒ½å›´ç»•ç€åŒä¸€ misinformation å†…å®¹å‚ä¸äº†ä¼ æ’­ï¼Œä½†è·¯å¾„ä¸åŒã€‚


```{r}
# Co-Quote Edges
co_quote_edges <- read_csv("data/mumin_csv/reply_quote_of_tweet.csv") |>
  filter(tgt %in% misinfo_tweet_id | src %in% misinfo_tweet_id) |> 
  rename(
    quoted_tweet_id = tgt,    # è¢«å¼•ç”¨çš„ tweet
    quoting_tweet_id = src    # å‘èµ·å¼•ç”¨çš„ tweet
  ) |> 
  # æ‰¾ quoting tweet æ˜¯è° retweeted çš„ï¼ˆå³ quoting userï¼‰
  left_join(retweet_map, by = c("quoting_tweet_id" = "tweet_id")) |> 
  rename(quoting_user = retweeter_id) |> 
  # æ‰¾ quoted tweet æ˜¯è° retweeted çš„ï¼ˆå³ quoted userï¼‰
  left_join(retweet_map, by = c("quoted_tweet_id" = "tweet_id")) |> 
  rename(quoted_user = retweeter_id) |> 
  filter(!is.na(quoting_user) & !is.na(quoted_user)) |> 
  mutate(
    user_1 = pmin(quoting_user, quoted_user),
    user_2 = pmax(quoting_user, quoted_user)
  ) |> 
  select(user_1, user_2) |> 
  group_by(user_1, user_2) |> 
  summarise(weight = n(), .groups = "drop") |> 
  mutate(type = "co-quote") |> 
  mutate(type = "co-quote") |>
  filter(user_1 != user_2) |>
  mutate(
    user_1 = as.character(user_1),
    user_2 = as.character(user_2)
  )

# Step 1: è·å–å®Œæ•´ retweeter åˆ—è¡¨ï¼ˆåŒ…å«æœ‰æ—  co-quote è¡Œä¸ºçš„ï¼‰
all_retweeters <- unique(c(
  covid_wuhan_misinfo$retweeter_id,  # æ ¸å¿ƒä¼ æ’­è€…
  co_quote_edges$user_1, 
  co_quote_edges$user_2
)) %>% 
  as.character() %>% 
  sort()

# Step 2: åˆ›å»ºå®Œæ•´çŸ©é˜µ
adj_matrix_co_quote <- acast(
  co_quote_edges, 
  user_1 ~ user_2, 
  value.var = "weight", 
  fill = 0
)

# Step 3: æ·»åŠ ç¼ºå¤±è¡Œå’Œåˆ—
missing_rows <- setdiff(all_retweeters, rownames(adj_matrix_co_quote))
missing_cols <- setdiff(all_retweeters, colnames(adj_matrix_co_quote))

if (length(missing_rows) > 0) {
  adj_matrix_co_quote <- rbind(
    adj_matrix_co_quote, 
    matrix(0, nrow = length(missing_rows), ncol = ncol(adj_matrix_co_quote), 
           dimnames = list(missing_rows, colnames(adj_matrix_co_quote)))
  )
}

if (length(missing_cols) > 0) {
  adj_matrix_co_quote <- cbind(
    adj_matrix_co_quote, 
    matrix(0, nrow = nrow(adj_matrix_co_quote), ncol = length(missing_cols), 
           dimnames = list(rownames(adj_matrix_co_quote), missing_cols))
  )
}

# Step 4: ç¡®ä¿çŸ©é˜µå¯¹é½
adj_matrix_co_quote <- adj_matrix_co_quote[all_retweeters, all_retweeters]

# ä¿å­˜ä¸º CSV
write.csv(adj_matrix_co_quote, "data/adj_matrix_co_quote.csv", row.names = TRUE)


nrow(adj_matrix_co_quote)
ncol(adj_matrix_co_quote)


```

### Co-reply

æˆ‘ä»¬èƒ½ç¡®å®šå…³ç³»çš„æ˜¯retweeter of replying tweetå’Œretweeter of replyed tweetä¹‹é—´çš„å…³ç³»

A retweeted äº†ä¸€ä¸ª replying tweetï¼ˆå¯¹ä¸€æ¡ misinformation tweet è¿›è¡Œäº† reply çš„ tweetï¼‰ã€‚

B retweeted äº†è¢« reply çš„é‚£æ¡ misinformation tweetã€‚

â†’ è¿ A å’Œ Bã€‚

ç°å®æ„ä¹‰ï¼š

A é€šè¿‡ retweet äº†ä¸€ä¸ª reply å¸–å­ï¼Œå‚ä¸äº†å›´ç»• misinformation çš„è®¨è®ºé“¾ã€‚

B æ˜¯ç›´æ¥çš„ä¼ æ’­è€…ã€‚

ä»–ä»¬æ˜¯å›´ç»•ç›¸åŒ misinformation å†…å®¹çš„ é—´æ¥ä¼ æ’­å‚ä¸è€…ã€‚

#### co_reply_edges Explanation (General Co-Reply Network)
This adjacency matrix captures the co-reply relationship between users who have engaged with each other via the reply function on Twitter in the context of misinformation.
In this network, two users are connected if:

One replied to a tweet, and

The other either replied to the same tweet or authored the replied tweet.

However, due to limitations in the dataset (missing tweet author information), this matrix primarily reflects shared engagement with the same tweet rather than direct author-replier relationships.

Note: In this case, no edges were detected, possibly because the retweeters of misinformation tweets did not participate significantly in reply activities.

```{r}
# Co-reply
co_reply_edges <- read_csv("data/mumin_csv/reply_reply_to_tweet.csv") |>
  filter(tgt %in% misinfo_tweet_id | src %in% misinfo_tweet_id) |> 
  rename(
    replied_tweet_id = tgt, 
    replying_tweet_id = src
  ) |> 
  # æ‰¾å‡ºä¸¤æ¡ tweet å„è‡ªçš„ retweeter
  left_join(retweet_map, by = c("replied_tweet_id" = "tweet_id")) |> 
  rename(user_replied = retweeter_id) |> 
  left_join(retweet_map, by = c("replying_tweet_id" = "tweet_id")) |> 
  rename(user_replying = retweeter_id) |> 
  filter(!is.na(user_replied) & !is.na(user_replying)) |> 
  mutate(
    user_1 = pmin(user_replied, user_replying),
    user_2 = pmax(user_replied, user_replying)
  ) |> 
  select(user_1, user_2) |> 
  group_by(user_1, user_2) |> 
  summarise(weight = n(), .groups = "drop") |> 
  mutate(type = "co-reply")

table(co_reply_edges$weight)

reply_data <- read_csv("data/mumin_csv/reply_reply_to_tweet.csv")
sum(reply_data$src %in% retweet_map$tweet_id)  # æœ‰å¤šå°‘ replying tweets è¢« retweetedï¼Ÿ
sum(reply_data$tgt %in% retweet_map$tweet_id)  # æœ‰å¤šå°‘ replied tweets è¢« retweetedï¼Ÿ


```

#### co_reply_edges_v2 Explanation (Focused Co-Reply on Misinformation Tweets)

This adjacency matrix focuses exclusively on replies to misinformation tweets.
It captures the relationship between users who retweeted the same misinformation tweet and also replied to it.

In this network, two users are connected if they both retweeted a misinformation tweet and subsequently replied to that tweet or engaged in discussions around it.

This captures shared attention and engagement specifically around misinformation content, providing insight into how users might form conversational clusters around these topics.

```{r}
co_reply_edges_v2 <- read_csv("data/mumin_csv/reply_reply_to_tweet.csv") |>
  filter(tgt %in% misinfo_tweet_id) |>  # è¢« reply çš„ tweet æ˜¯ misinformation
  rename(replied_tweet_id = tgt) |> 
  # æ‰¾å‡ºè¿™ä¸ª misinformation tweet è¢«è° retweeted è¿‡
  left_join(retweet_map, by = c("replied_tweet_id" = "tweet_id")) |>
  rename(user_replied = retweeter_id) |> 
  filter(!is.na(user_replied)) |> 
  group_by(replied_tweet_id) |> 
  summarise(
    pairs = list(as.data.frame(t(combn(user_replied, 2)))), 
    .groups = "drop"
  ) |> 
  unnest(pairs) |> 
  mutate(
    user_1 = pmin(V1, V2),
    user_2 = pmax(V1, V2)
  ) |> 
  select(user_1, user_2) |> 
  group_by(user_1, user_2) |> 
  summarise(weight = n(), .groups = "drop") |> 
  mutate(type = "co-reply-v2")  |>
  filter(user_1 != user_2) |>
  mutate(
    user_1 = as.character(user_1),
    user_2 = as.character(user_2)
  )

# Step 1: å®šä¹‰æ‰€æœ‰ retweetersï¼ˆåŒ…æ‹¬å‚ä¸å’Œæœªå‚ä¸ reply çš„ï¼‰
all_retweeters_reply <- unique(c(
  covid_wuhan_misinfo$retweeter_id,  # æ ¸å¿ƒä¼ æ’­è€…
  co_reply_edges_v2$user_1, 
  co_reply_edges_v2$user_2
)) %>% 
  as.character() %>% 
  sort()

# Step 2: åˆ›å»º Adjacency Matrix
adj_matrix_co_reply_v2 <- acast(
  co_reply_edges_v2, 
  user_1 ~ user_2, 
  value.var = "weight", 
  fill = 0
)

# Step 3: ç¡®ä¿çŸ©é˜µåŒ…å«æ‰€æœ‰ retweeters
missing_rows <- setdiff(all_retweeters_reply, rownames(adj_matrix_co_reply_v2))
missing_cols <- setdiff(all_retweeters_reply, colnames(adj_matrix_co_reply_v2))

if (length(missing_rows) > 0) {
  adj_matrix_co_reply_v2 <- rbind(
    adj_matrix_co_reply_v2, 
    matrix(0, nrow = length(missing_rows), ncol = ncol(adj_matrix_co_reply_v2), 
           dimnames = list(missing_rows, colnames(adj_matrix_co_reply_v2)))
  )
}

if (length(missing_cols) > 0) {
  adj_matrix_co_reply_v2 <- cbind(
    adj_matrix_co_reply_v2, 
    matrix(0, nrow = nrow(adj_matrix_co_reply_v2), ncol = length(missing_cols), 
           dimnames = list(rownames(adj_matrix_co_reply_v2), missing_cols))
  )
}

# Step 4: Reorder to ensure rows and columns match
adj_matrix_co_reply_v2 <- adj_matrix_co_reply_v2[all_retweeters_reply, all_retweeters_reply]

# Step 5: ä¿å­˜ CSV
write.csv(adj_matrix_co_reply_v2, "data/adj_matrix_co_reply_v2.csv", row.names = TRUE)

nrow(adj_matrix_co_reply_v2)
ncol(adj_matrix_co_reply_v2)


```

This step ensures that the adjacency matrix fully represents the social structure by including all users who participated in the retweet network, regardless of their direct involvement in reply behaviors.

Users who did not engage in replying are represented as isolated nodes (zero rows and columns), preserving the completeness of the network for structural analysis.


### Follow

#### Undirected follow

This edge represents an undirected tie between any two users who are both retweeters of misinformation tweets and have a recorded follower-followee relationship in either direction. However, the relationship is not necessarily mutual. It simply captures the existence of a connection in the follower network among the misinformation retweeters, regardless of direction.

srcï¼ˆuser_aï¼‰æ˜¯followerï¼Œtgtï¼ˆuser_bï¼‰æ˜¯followeeã€‚

filter(user_a %in% retweeter_id_misinfo & user_b %in% retweeter_id_misinfo)ï¼š
â†’ è¡¨ç¤ºè¿™æ¡è¾¹ä»…è¦æ±‚åŒæ–¹éƒ½æ˜¯ retweetersï¼Œä¸è¦æ±‚ä»–ä»¬äº’ç›¸å…³æ³¨ã€‚

pmin/pmax æ˜¯ä¸ºäº†å»æ‰æ–¹å‘æ€§ï¼ŒæŠŠè¾¹è½¬æˆæ— å‘çš„ã€‚

è¿™ä¸æ˜¯ mutual followï¼ˆäº’ç›¸å…³æ³¨ï¼‰ï¼Œè€Œæ˜¯åªè¦ follower å’Œ followee éƒ½æ˜¯ retweeterï¼Œå°±åˆ›å»ºä¸€æ¡æ— å‘è¾¹ã€‚

```{r}
# Follow
# è·å– retweeter åˆ—è¡¨
retweeter_id_misinfo <- unique(covid_wuhan_misinfo$retweeter_id)

# æ„é€  Follow è¾¹
retweeter_follow_edges <- read_csv("data/mumin_csv/user_follows_user.csv") |>
  rename(user_a = src, user_b = tgt) |> 
  # ä¿ç•™ä¸¤è€…éƒ½åœ¨ retweeter åˆ—è¡¨ä¸­çš„è®°å½•
  filter(user_a %in% retweeter_id_misinfo & user_b %in% retweeter_id_misinfo) |> 
  mutate(
    user_1 = pmin(user_a, user_b),  # ä¿æŒå°çš„ ID åœ¨å‰ï¼Œå»æ‰æ–¹å‘æ€§
    user_2 = pmax(user_a, user_b)
  ) |> 
  select(user_1, user_2) |> 
  distinct() |>  # å»é‡ï¼Œé˜²æ­¢é‡å¤è®°å½• A -> B å’Œ B -> A
  mutate(weight = 1, type = "follow")

retweeter_follow_edges

```

all 0 matrix:

```{r}
# è·å–æ‰€æœ‰ retweeters
all_retweeters <- sort(unique(c(
  covid_wuhan_misinfo$retweeter_id, 
  retweeter_follow_edges$user_1, 
  retweeter_follow_edges$user_2
))) %>% as.character()

# åˆ›å»ºå…¨ 0 çŸ©é˜µ
adj_matrix_follow <- matrix(
  0, 
  nrow = length(all_retweeters), 
  ncol = length(all_retweeters),
  dimnames = list(all_retweeters, all_retweeters)
)

# è½¬æˆæ•°æ®æ¡†å¯¼å‡º
write.csv(adj_matrix_follow, "data/adj_matrix_follow.csv", row.names = TRUE)

```

#### Directed follow

This edge preserves the directionality of follower relationships between misinformation retweeters (e.g., User A follows User B). It allows for modeling influence flow and assessing asymmetric structures (e.g., hubs or hierarchies) in the spread of misinformation.

follower â†’ followee è¡¨ç¤ºå•å‘çš„å…³æ³¨ã€‚

åŒæ ·åœ°ï¼Œåªè¦ä¸¤è€…éƒ½åœ¨ retweeter åˆ—è¡¨ä¸­ï¼Œå°±ä¿ç•™è¿™ä¸ªæ–¹å‘æ€§å…³ç³»ã€‚

è¿™æ˜¯çœŸæ­£çš„ directed edgeï¼Œè¡¨ç¤ºâ€œè°å…³æ³¨äº†è°â€ï¼Œæ–¹å‘æ€§ä¿ç•™ã€‚

```{r}
retweeter_follow_edges_directed <- read_csv("data/mumin_csv/user_follows_user.csv") |> 
  rename(follower = src, followee = tgt) |> 
  # åªä¿ç•™ retweeter ä¹‹é—´çš„å…³æ³¨å…³ç³»ï¼ˆæœ‰æ–¹å‘ï¼‰
  filter(follower %in% retweeter_id_misinfo & followee %in% retweeter_id_misinfo) |> 
  mutate(
    weight = 1,          # é»˜è®¤æƒé‡ä¸º 1
    type = "follow_directed"      # æ˜ç¡®è¯´æ˜æ˜¯å…³æ³¨å…³ç³»
  ) |> 
  select(follower, followee, weight, type)

```


```{r}
follow_data<-read_csv("data/mumin_csv/user_follows_user.csv") 
sum(follow_data$src %in% covid_wuhan_misinfo$retweeter_id)
sum(follow_data$tgt %in% covid_wuhan_misinfo$retweeter_id)

sum(covid_wuhan_misinfo$retweeter_id%in% follow_data$src)
sum(covid_wuhan_misinfo$retweeter_id%in% follow_data$tgt)

covid_wuhan_misinfo |>
  group_by(date, keywords) |>
  select(date, keywords) |>
  arrange(date)|>
  unique()

```



# Analysis

## Co-retweet Net

```{r}
# è¯»å– co-retweet adjacency matrix
adj_matrix_co_retweet <- read.csv(
  "data/adj_matrix_co_retweet.csv", 
  row.names = 1, 
  check.names = FALSE  # å…³é”®å‚æ•°ï¼Œç¦æ­¢è‡ªåŠ¨åŠ  x å‰ç¼€
)

# æ„å»º network å¯¹è±¡ï¼ˆæ— å‘å›¾ï¼‰
net_co_retweet <- network(as.matrix(adj_matrix_co_retweet), directed = FALSE, matrix.type = "adjacency")

get.vertex.attribute(net_co_retweet, "vertex.names")

# è·å–åˆ†ç»„ä¿¡æ¯
retweeter_group <- covid_wuhan_misinfo %>%
  mutate(retweeter_id = as.character(retweeter_id)) %>%
  select(retweeter_id, date, keywords) %>%
  distinct() %>%
  arrange(date) %>%
  mutate(group = as.numeric(factor(paste(date, keywords), levels = unique(paste(date, keywords)))))

# å°† Group ä¿¡æ¯æ·»åŠ åˆ°èŠ‚ç‚¹å±æ€§ä¸­
set.vertex.attribute(
  net_co_retweet, 
  "group", 
  retweeter_group$group[match(get.vertex.attribute(net_co_retweet, "vertex.names"), retweeter_group$retweeter_id)]
)

get.vertex.attribute(net_co_retweet, "group")

for (g in 1:6) {
  sub_g <- get.inducedSubgraph(net_co_retweet, 
                               which(get.vertex.attribute(net_co_retweet, "group") == g))
  plot.network(sub_g, 
                displaylabels = FALSE, 
                vertex.cex = 1, 
                vertex.col = "skyblue", 
                main = paste("Subgraph for Group", g))
}


```

```{r}
# 1. è·å–é˜¶æ®µä¿¡æ¯
group_stages <- sort(unique(get.vertex.attribute(net_co_retweet, "group")))

results <- data.frame()  # æ¸…ç©ºæˆ–åˆå§‹åŒ–

for (g in group_stages) {
  sub_nodes <- which(get.vertex.attribute(net_co_retweet, "group") <= g)
  sub_g <- get.inducedSubgraph(net_co_retweet, v = sub_nodes)
  
  adj_mat <- as.matrix.network(sub_g, matrix.type = "adjacency")
  
  results <- rbind(results, data.frame(
    Stage = g,
    Nodes = network.size(sub_g),
    Edges = network.edgecount(sub_g),
    Density = gden(sub_g),
    Degree_Centralization = centralization(adj_mat, FUN = degree, mode = "graph"),
    Closeness_Centralization = centralization(adj_mat, FUN = closeness, mode = "graph"),
    Betweenness_Centralization = centralization(adj_mat, FUN = betweenness, mode = "graph"),
    Transitivity = gtrans(sub_g, mode = "graph")
  ))
}
# 4. æŸ¥çœ‹ç»“æœ
print(results)
```


1ï¸âƒ£ å¯†åº¦ (Density) ä¸‹é™æ¸…æ™°å¯è§
ç¬¬ä¸€é˜¶æ®µå¯†åº¦ = 1.0ï¼Œå®Œå…¨å›¾ï¼ˆæ¯ä¸ªèŠ‚ç‚¹éƒ½äº’ç›¸å…³è”ï¼‰ã€‚

éšç€èŠ‚ç‚¹å¢åŠ ï¼Œå¯†åº¦è¿…é€Ÿä¸‹é™åˆ° 0.13 å·¦å³ã€‚

ğŸ“Œ è§£é‡Šï¼š

æœ€åˆçš„æ ¸å¿ƒä¼ æ’­è€…é«˜åº¦äº’åŠ¨ï¼Œå½¢æˆäº†å°è§„æ¨¡çš„â€œç²¾è‹±ä¼ æ’­åœˆâ€ï¼ˆå¦‚å¯†é—­å°å›¢ä½“ï¼‰ã€‚

éšç€ä¼ æ’­æ‰©æ•£ï¼Œè¾¹ç¼˜ç”¨æˆ·ï¼ˆå¤–å›´ retweetersï¼‰è¢«é€æ¸å¸å¼•è¿›æ¥ï¼Œä½†ä»–ä»¬ä¹‹é—´å¹¶æ²¡æœ‰å¤§é‡çš„ç›¸äº’è”ç³»ï¼Œä»…ä¸æ ¸å¿ƒä¼ æ’­è€…æˆ–å°‘æ•°ç”¨æˆ·è¿æ¥ï¼Œå¯¼è‡´ç½‘ç»œè¶Šæ¥è¶Šç¨€ç–ã€‚

2ï¸âƒ£ Degree Centralization ç¨å¾®ä¸Šå‡ï¼Œä½†å§‹ç»ˆåä½
ä» 0.048 ç¨³å®šä¸Šå‡åˆ° 0.086ã€‚

è¡¨æ˜ï¼š

è™½ç„¶æœ‰éƒ¨åˆ†â€œè¶…çº§ä¼ æ’­è€…â€èŠ‚ç‚¹å‡ºç°ï¼ˆæ‹¥æœ‰æ›´å¤šè¿æ¥ï¼‰ï¼Œ

ä½†æ•´ä¸ªç½‘ç»œå¹¶æ²¡æœ‰å½¢æˆå¼ºçƒˆçš„â€œå•ä¸€æ ¸å¿ƒæ§åˆ¶â€ç°è±¡ï¼Œè¿˜æ˜¯è¾ƒä¸ºåˆ†æ•£çš„ã€‚

ğŸ“Œ å¯¹æ¯”åˆ†æï¼š

è¿™ç¬¦åˆè°£è¨€ä¼ æ’­ä¸­çš„â€œå»ä¸­å¿ƒåŒ–ç‰¹å¾â€ï¼š

è™½ç„¶æœ‰å½±å“åŠ›å¤§çš„ç”¨æˆ·ï¼Œä½†ä¹Ÿæœ‰å¤§é‡ä¸­å°èŠ‚ç‚¹å‚ä¸ä¼ æ’­ï¼Œå½¢æˆäº†â€œåˆ†å¸ƒå¼æ‰©æ•£â€ã€‚

3ï¸âƒ£ Closeness å’Œ Betweenness Centralization ä¸€ç›´ä¸º 0
ğŸ“Œ è§£é‡Šï¼š

è¿™æ„å‘³ç€ï¼š

æ²¡æœ‰èŠ‚ç‚¹çœŸæ­£æ‰¿æ‹…â€œä¸­ä»‹æ¡¥æ¢â€çš„è§’è‰²ï¼ˆbetweenness = 0ï¼‰ã€‚

æ²¡æœ‰èŠ‚ç‚¹åœ¨ä¿¡æ¯ä¼ é€’è·¯å¾„ä¸­å æ®æ˜¾è‘—ä½ç½®ï¼ˆcloseness = 0ï¼‰ã€‚

å¾ˆå¯èƒ½æ˜¯å› ä¸ºè¿™ä¸ªç½‘ç»œæ˜¯ æ— å‘å›¾ä¸”å°å›¢ä½“å†…éƒ¨å¼ºè¿æ¥ï¼Œå¯¼è‡´å¤§éƒ¨åˆ†èŠ‚ç‚¹è·¯å¾„é•¿åº¦ç›¸ä¼¼ï¼Œæˆ–æ ¹æœ¬ä¸éœ€è¦é€šè¿‡ä¸­ä»‹èŠ‚ç‚¹å°±èƒ½ç›´æ¥è¿æ¥ã€‚

ğŸ“Œ æ´å¯Ÿï¼š

è¿™æ˜¯å…¸å‹çš„â€œç¾¤å›¢å¼ä¼ æ’­â€ï¼Œä¸åŒç¾¤ä½“å†…éƒ¨è¿é€šç´§å¯†ï¼Œä½†ç¾¤ä½“ä¹‹é—´çš„æ¡¥æ¢èŠ‚ç‚¹ç¨€ç¼ºã€‚

ç¬¦åˆ Echo Chamber Theory çš„æè¿° â€”â€” ä¿¡æ¯åœ¨ç¾¤ä½“å†…éƒ¨åå¤ä¼ æ’­ï¼Œå¾ˆéš¾è·¨ç¾¤ä½“ä¼ é€’ã€‚

4ï¸âƒ£ Transitivity (å…¨å±€èšç±»ç³»æ•°) æé«˜
ä¸€ç›´ç»´æŒåœ¨ æ¥è¿‘ 1.0ï¼Œå“ªæ€•èŠ‚ç‚¹æ•°é‡å¢åŠ ä¹Ÿåªæœ‰æå¾®å¼±ä¸‹é™ã€‚

ğŸ“Œ è§£é‡Šï¼š

ç½‘ç»œä¸­ç»å¤§éƒ¨åˆ†ä¸‰å…ƒé—­ç¯éƒ½å­˜åœ¨ â€”â€” æ¢å¥è¯è¯´ï¼Œâ€œæœ‹å‹çš„æœ‹å‹ä¹Ÿæ˜¯æœ‹å‹â€ã€‚

å†æ¬¡å°è¯äº†é«˜åº¦ç¾¤ä½“åŒ–ã€å°åœˆå­å¼ºè¿æ¥çš„ä¼ æ’­æ¨¡å¼ã€‚

ğŸ“Š ç»¼åˆæ´å¯Ÿæ€»ç»“
åˆæœŸæ ¸å¿ƒå°å›¢ä½“ä¸»å¯¼ï¼Œéšç€ä¼ æ’­æ¼”åŒ–ï¼Œç½‘ç»œè§„æ¨¡æ‰©å¤§ä½†è”ç³»ç¨€ç–ã€‚

ç½‘ç»œä¿æŒé«˜åº¦çš„èšç±»å’Œå±€éƒ¨æ€§ï¼Œç¾¤ä½“ä¹‹é—´çš„ä¼ æ’­å£å’å¼ºçƒˆã€‚

ä¿¡æ¯çš„è·¨ç¾¤ä½“æ‰©æ•£èƒ½åŠ›è¾ƒå¼±ï¼Œç¬¦åˆ å›éŸ³å®¤æ•ˆåº”ï¼ˆEcho Chamberï¼‰ å’Œ å»ä¸­å¿ƒåŒ–çš„åˆ†æ•£ä¼ æ’­ç‰¹å¾ã€‚

æ²¡æœ‰å½¢æˆå…¸å‹çš„â€œæ˜æ˜Ÿç”¨æˆ·ä¸»å¯¼çš„ä¸­å¿ƒåŒ–æ‰©æ•£â€ï¼Œè€Œæ˜¯å¤šç¾¤ä½“ã€å°åœˆå­åœ¨ç‹¬ç«‹ä¼ æ’­ã€‚


```{r}
results <- data.frame()  # æ¸…ç©ºæˆ–åˆå§‹åŒ–

for (g in group_stages) {
  sub_nodes <- which(get.vertex.attribute(net_co_retweet, "group") == g)
  sub_g <- get.inducedSubgraph(net_co_retweet, v = sub_nodes)
  
  adj_mat <- as.matrix.network(sub_g, matrix.type = "adjacency")
  
  results <- rbind(results, data.frame(
    Stage = g,
    Nodes = network.size(sub_g),
    Edges = network.edgecount(sub_g),
    Density = gden(sub_g),
    Degree_Centralization = centralization(adj_mat, FUN = degree, mode = "graph"),
    Closeness_Centralization = centralization(adj_mat, FUN = closeness, mode = "graph"),
    Betweenness_Centralization = centralization(adj_mat, FUN = betweenness, mode = "graph"),
    Transitivity = gtrans(sub_g, mode = "graph")
  ))
}
# 4. æŸ¥çœ‹ç»“æœ
print(results)


```


1ï¸âƒ£ åˆå§‹é˜¶æ®µ (Stage 1)
å®Œç¾çš„ å®Œå…¨å›¾ (Density=1)ï¼ŒèŠ‚ç‚¹ä¹‹é—´å…¨è¿æ¥ã€‚

å„ç§ä¸­å¿ƒæ€§ä¸º 0 â†’ æƒåŠ›æåº¦å‡è¡¡ï¼Œæ²¡æœ‰æ˜æ˜¾çš„æ ¸å¿ƒèŠ‚ç‚¹ã€‚

Transitivity = 1 â†’ å®Œç¾çš„ä¸‰è§’é—­ç¯ï¼Œè¿™é€šå¸¸å‘ç”Ÿåœ¨å°å›¢ä½“ã€åŒè´¨æ€§æé«˜çš„åœˆå­ã€‚

2ï¸âƒ£ ä¼ æ’­çˆ†å‘ (Stage 2)
Density é™åˆ° 0.51ï¼Œä½†ç½‘ç»œè§„æ¨¡æ‰©å¤§ï¼ŒEdges å¢å¤šã€‚

Degree Centralization å¼€å§‹ä¸Šå‡ï¼Œè¡¨æ˜å‡ºç°äº†å…³é”®ä¼ æ’­è€…ï¼ˆæ ¸å¿ƒèŠ‚ç‚¹é€æ¸å½¢æˆï¼‰ã€‚

ä¾ç„¶æ˜¯é«˜é—­åŒ…ï¼ˆTransitivity=1ï¼‰ï¼Œè¯´æ˜ç¤¾ç¾¤å†…éƒ¨è”ç³»ç´§å¯†ï¼Œä½†å¼€å§‹æ˜¾ç°ä¼ æ’­åˆ†å·¥ã€‚

3ï¸âƒ£ çŸ­æš‚æ”¶ç¼© (Stage 3)
èŠ‚ç‚¹æ•°å’Œè¾¹æ•°å¤§å¹…å‡å°‘ï¼Œç½‘ç»œå†æ¬¡å˜æˆå®Œå…¨å›¾ã€‚

å¯èƒ½æ˜¯ä¿¡æ¯ä¼ æ’­çš„ çŸ­æš‚åœæ» æˆ– å°åœˆå±‚ç‹¬ç«‹ä¼ æ’­ã€‚

4ï¸âƒ£ æ‰©å±•å°è¯• (Stage 4)
èŠ‚ç‚¹æ•°å†æ¬¡ä¸Šå‡ï¼Œä½† Density å¾ˆä½ï¼ˆ0.24ï¼‰ï¼Œç½‘ç»œæ¾æ•£ã€‚

è¯´æ˜ä¿¡æ¯æ‰©æ•£åˆ°äº†æ›´å¹¿æ³›çš„å—ä¼—ï¼Œä½†å½¼æ­¤ä¹‹é—´è”ç³»ä¸å¼ºã€‚

ä¸­å¿ƒæ€§ä»ä½ï¼Œæœªå½¢æˆç¨³å®šçš„ä¿¡æ¯ä¼ æ’­ä¸»å¯¼è€…ã€‚

5ï¸âƒ£ æ ¸å¿ƒæ‰©æ•£ (Stage 5)
è™½ç„¶èŠ‚ç‚¹æ•°ä¸å¤šï¼Œä½†å‡ºç°äº†æ˜¾è‘—çš„ä¸­å¿ƒæ€§ç‰¹å¾ï¼š

Degree Centralization é«˜è¾¾ 0.36

Closeness Centralization è¾¾åˆ° 0.47

Betweenness Centralization ä¹Ÿæœ‰ 0.16

è¿™ä¸€é˜¶æ®µå¯èƒ½å‡ºç°äº†å…³é”®æ„è§é¢†è¢–æˆ–è¶…çº§ä¼ æ’­è€…ï¼Œå¯¹ä¿¡æ¯æ‰©æ•£èµ·åˆ°æ˜¾è‘—ä½œç”¨ã€‚

Transitivity é™ä½åˆ° 0.85 â†’ ç½‘ç»œå†…é—­ç¯å‡å°‘ï¼Œæ›´å¤š æ¡¥æ¥å‹èŠ‚ç‚¹ å‡ºç°ã€‚

6ï¸âƒ£ æœ€æœ«é˜¶æ®µ (Stage 6)
ç½‘ç»œå†æ¬¡æ”¶ç¼©æˆå®Œå…¨å›¾ï¼Œç¾¤ä½“éå¸¸å°ï¼Œä½†è”ç³»éå¸¸ç´§å¯†ã€‚

å¯èƒ½æ˜¯æ ¸å¿ƒç¾¤ä½“çš„ä½™æ³¢äº¤æµï¼Œä½†æœªå†ç»§ç»­å‘å¤–ä¼ æ’­ã€‚

## tERGM sample

```{r}

# # Step 1: å‡†å¤‡ç½‘ç»œåˆ—è¡¨ï¼ˆæ¯ä¸ªæ—¶é—´ç‚¹ä¸€ä¸ª networkï¼‰
# network_list <- list()
# for (g in group_stages) {
#   sub_nodes <- which(get.vertex.attribute(net_co_retweet, "group") <= g)
#   sub_g <- get.inducedSubgraph(net_co_retweet, v = sub_nodes)
#   network_list[[as.character(g)]] <- sub_g
# }
# 
# # Step 2: æ„å»ºæ¨¡å‹
# # ä½¿ç”¨ç®€å•ç»“æ„æ€§ç‰¹å¾ä½œä¸ºä¾‹å­
# model <- tergm(
#   network_list ~ Form(~edges + gwesp(0.5, fixed = TRUE)) + 
#     Dissolution(~edges), 
#   estimate = "CMLE"
# )
# 
# summary(model)

```


# Reference


# Appendix

Raw dataset download link: <https://data.bristol.ac.uk/data/en_GB/dataset/23yv276we2mll25fjakkfim2ml/resource/e63a98c4-b4d6-4a89-9be9-06875b83ce33?inner_span=True>


After downloading all files, create a `data` folder in your working directory and place the files inside it.

Follow the setup instructions provided [here](https://colab.research.google.com/drive/1JCjgg3moGBOuZk4iVjBpQNqgsAYFyNoS#scrollTo=YS1uxj-59UmA). Use the shell to install required packages via `pip` as directed.

Next, open `VS Code`, and run the following Python script to convert `.pkl` files to `.csv`:

```{python}
#| eval: false
#| include: true

import pandas as pd
import os

input_dir = "data/mumin"
output_dir = "data/mumin_csv"
os.makedirs(output_dir, exist_ok=True)

file_names = [
    "claim",
    "user",
    "tweet",
    "reply",
    "article",
    "tweet_discusses_claim",
    "article_discusses_claim",
    "user_follows_user",
    "user_retweeted_tweet",
    "reply_quote_of_tweet",
    "reply_reply_to_tweet"
]

for name in file_names:
    input_path = os.path.join(input_dir, name)
    output_path = os.path.join(output_dir, f"{name}.csv")
    try:
        df = pd.read_pickle(input_path, compression="xz")
        df.to_csv(output_path, index=False)
        print(f"Saved: {output_path}")
    except Exception as e:
        print(f"Failed to convert {name}: {e}")

```

This script will generate the corresponding CSV files for further analysis.

Then, preview the Data in R:

```{r}
#| eval: false
#| include: true
# Set the data path 
data_path <- "data/mumin_csv/"

# Define the list of files to read
files <- c(
  "claim.csv", "user.csv", "tweet.csv", "reply.csv", "article.csv",
  "tweet_discusses_claim.csv", "article_discusses_claim.csv",
  "user_follows_user.csv", "user_retweeted_tweet.csv",
  "reply_quote_of_tweet.csv", "reply_reply_to_tweet.csv"
)

# Preview the first few rows of each file 
previews <- lapply(files, function(fname) {
  fpath <- file.path(data_path, fname)
  tryCatch(
    read_csv(fpath, n_max = 5),
    error = function(e) tibble::tibble(error = paste("Error:", e$message))
  )
})

names(previews) <- files

```

```{=html}
<script>
document.addEventListener("DOMContentLoaded", function() {
    const toc = document.getElementById("TOC");
    if (toc) {
        const sourceLink = document.createElement("div");
        sourceLink.innerHTML = `
            <div class="toc-source">
                <a href="https://github.com/troy-yu-cheng/sna-final-paper" 
                   target="_blank" 
                   class="github-button">
                   <svg xmlns="http://www.w3.org/2000/svg" 
                        viewBox="0 0 24 24" 
                        width="16" 
                        height="16" 
                        fill="currentColor"
                        style="vertical-align: middle; margin-right: 5px;">
                     <path d="M12 0C5.373 0 0 5.373 0 12c0 5.303 3.438 9.8 8.207 11.387.6.113.82-.26.82-.577v-2.157c-3.338.726-4.033-1.416-4.033-1.416-.546-1.386-1.332-1.756-1.332-1.756-1.09-.745.083-.73.083-.73 1.205.084 1.84 1.237 1.84 1.237 1.07 1.832 2.807 1.303 3.492.996.108-.774.418-1.303.76-1.602-2.665-.3-5.466-1.332-5.466-5.93 0-1.311.468-2.382 1.237-3.222-.124-.302-.536-1.52.118-3.163 0 0 1.008-.322 3.3 1.23a11.516 11.516 0 0 1 3.002-.403 11.486 11.486 0 0 1 3.002.403c2.292-1.552 3.3-1.23 3.3-1.23.654 1.644.242 2.861.118 3.163.77.84 1.236 1.911 1.236 3.222 0 4.61-2.807 5.627-5.48 5.922.43.372.812 1.103.812 2.222v3.293c0 .321.218.694.825.576C20.565 21.796 24 17.3 24 12 24 5.373 18.627 0 12 0z"/>
                   </svg>
                   View source
                </a>
            </div>
        `;
        toc.appendChild(sourceLink);
    }
});
</script>
```

