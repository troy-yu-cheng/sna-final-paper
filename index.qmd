---
title: "Social Network Analysis"
subtitle: "Final Paper"
author: 
  - name: "Troy (Yu) Cheng"
    email: yc1317@georgetown.edu
    affiliation: Georgetown University
    corresponding: true
df-print: kable
title-block-banner: "#0a0e1a"
title-block-banner-color: "#4DB8FF"
execute:
  warning: false
date: 2025-04-28
date-modified: last-modified
format:
  html:
    embed-resources: true
    toc: true                 
    toc-title: "Contents"     
    toc-location: right       
    number-sections: true
    number-depth: 3       
    smooth-scroll: true       
    css: trycstyle.css 
    code-overflow: wrap
include-in-header:
  text: |
    <link rel="shortcut icon" href="assets/gu.ico" type="image/x-icon">           
highlight-style: nord       
engine: knitr
---
# Abstract

我现在对covid病毒是在中国/wuhan爆发这条谣言在推特用户间的传播规律（随时间）很感兴趣
一个misinformation的源头很难确定是具体哪个user/tweet 直接研究这种传播的过程会好一点
我们以retweeter作为节点，retweeter相互之间的co-retweet，co-reply，co-quote和co-follow作为边

# Literature Review


# Dataset

Raw dataset download link: <https://data.bristol.ac.uk/data/en_GB/dataset/23yv276we2mll25fjakkfim2ml/resource/e63a98c4-b4d6-4a89-9be9-06875b83ce33?inner_span=True>

## Set up

```{r}
#| label: set up packages
library(tidyverse)
library(readr)
library(statnet)
library(intergraph)
```


```{r}
# Load data
tweets <- read_csv("data/mumin_csv/tweet.csv")
claims <- read_csv("data/mumin_csv/claim.csv")
tweet_claim <- read_csv("data/mumin_csv/tweet_discusses_claim.csv")
user_retweet <- read_csv("data/mumin_csv/user_retweeted_tweet.csv")

# Join tweet & claim via tweet_claim
tweet_claim_joined <- tweet_claim |> 
  rename(tweet_id = src, claim_id = tgt) |> 
  inner_join(claims, by = c("claim_id" = "id"))

# Join retweeters within users & claim via retweeted tweet
user_claim <- user_retweet |> 
  rename(retweeter_id = src, tweet_id = tgt) |> 
  inner_join(tweet_claim_joined, by = "tweet_id")

retweet_map <- user_claim |> 
  select(retweeter_id, tweet_id) |> 
  distinct() 

table(user_claim$label)
table(user_claim$cluster_keywords)
```

初步整合数据之后，我们进一步筛选我们要研究的特定的misinformation话题

```{r}
user_claim_filtered <- user_claim |> 
  filter(str_detect(keywords, "china|wuhan")) 

unique(user_claim_filtered$keywords)
```

After checking the keywords, 我手动选择了所有跟covid19是中国/Wuhan蓄意制造和散播的misinformation。并把这些内容限制在了英文帖子。

```{r}
covid_wuhan_misinfo <- user_claim_filtered |>
  filter(keywords %in% c(
    "coronavirus comes biological laboratory wuhan",
    "including corona virus manufactured wuhan",
    "china created spread coronavirus",
    "announced coronavirus artificially created china",
    "china created coronavirus",
    "coronavirus created laboratory china",
    "honjo said china manufactured coronavirus",
    "said new coronavirus manufactured wuhan",
    "coronavirus tested biological weapon china",
    "china testing coronavirus biological weapon",
    "covid 19 lab china",
    "coronavirus natural worked wuhan laboratory",
    "coronavirus manufactured laboratory wuhan specifically",
    "doctor claimed coronavirus china create",
    "corona virus manufactured china",
    "says current coronavirus manufactured wuhan",
    "wuhan coronavirus created patented",
    "covid 19 disease manufactured china",
    "pandemic caused artificial virus china",
    "collaborator assures coronavirus manufactured wuhan",
    "concluded covid leaked wuhan lab",
    "coronavirus created united states china",
    "air comes china import coronavirus",
    "coronavirus released china"
  )) |>
  filter(language == "en") |>
  filter(label == "misinformation")

table(covid_wuhan_misinfo$keywords)
table(covid_wuhan_misinfo$cluster_keywords)

```

## Create SNA data

This network reflects the co-dissemination behavior of misinformation retweeters and their mutual social connections. While the true transmission paths are unknown, this co-spreader network allows us to model the structural properties and potential information diffusion routes among these users.

```{r}

# 获取 misinformation tweet ID
misinfo_tweet_id <- unique(covid_wuhan_misinfo$tweet_id)

# 保留转发这些 tweet 的所有 retweeter-pair（共传播者）
co_retweet_edges <- user_retweet |> 
  filter(tgt %in% misinfo_tweet_id) |> 
  rename(tweet_id = tgt, retweeter_id = src) |> 
  group_by(tweet_id) |> 
  summarise(
    pairs = list(as.data.frame(t(combn(retweeter_id, 2)))), 
    .groups = "drop"
  ) |> 
  unnest(pairs) |> 
  mutate(
    user_1 = pmin(V1, V2),  # 保持小的 ID 在前
    user_2 = pmax(V1, V2)   # 大的 ID 在后
  ) |> 
  select(user_1, user_2) |> 
  group_by(user_1, user_2) |> 
  summarise(weight = n(), .groups = "drop") |> 
  mutate(type = "co-retweet")

table(co_retweet_edges$weight)


# Co-Quote Edges
co_quote_edges <- read_csv("data/mumin_csv/reply_quote_of_tweet.csv") |>
  filter(tgt %in% misinfo_tweet_id | src %in% misinfo_tweet_id) |> 
  rename(
    quoted_tweet_id = tgt,    # 被引用的 tweet
    quoting_tweet_id = src    # 发起引用的 tweet
  ) |> 
  # 找 quoting tweet 是谁 retweeted 的（即 quoting user）
  left_join(retweet_map, by = c("quoting_tweet_id" = "tweet_id")) |> 
  rename(quoting_user = retweeter_id) |> 
  # 找 quoted tweet 是谁 retweeted 的（即 quoted user）
  left_join(retweet_map, by = c("quoted_tweet_id" = "tweet_id")) |> 
  rename(quoted_user = retweeter_id) |> 
  filter(!is.na(quoting_user) & !is.na(quoted_user)) |> 
  mutate(
    user_1 = pmin(quoting_user, quoted_user),
    user_2 = pmax(quoting_user, quoted_user)
  ) |> 
  select(user_1, user_2) |> 
  group_by(user_1, user_2) |> 
  summarise(weight = n(), .groups = "drop") |> 
  mutate(type = "co-quote")


table(co_quote_edges$weight)
```


```{r}
# Co-reply
co_reply_edges <- read_csv("data/mumin_csv/reply_reply_to_tweet.csv") |>
  filter(tgt %in% misinfo_tweet_id | src %in% misinfo_tweet_id) |> 
  rename(
    replied_tweet_id = tgt, 
    replying_tweet_id = src
  ) |> 
  # 找出两条 tweet 各自的 retweeter
  left_join(retweet_map, by = c("replied_tweet_id" = "tweet_id")) |> 
  rename(user_replied = retweeter_id) |> 
  left_join(retweet_map, by = c("replying_tweet_id" = "tweet_id")) |> 
  rename(user_replying = retweeter_id) |> 
  filter(!is.na(user_replied) & !is.na(user_replying)) |> 
  mutate(
    user_1 = pmin(user_replied, user_replying),
    user_2 = pmax(user_replied, user_replying)
  ) |> 
  select(user_1, user_2) |> 
  group_by(user_1, user_2) |> 
  summarise(weight = n(), .groups = "drop") |> 
  mutate(type = "co-reply")

table(co_reply_edges$weight)

reply_data <- read_csv("data/mumin_csv/reply_reply_to_tweet.csv")
sum(reply_data$src %in% retweet_map$tweet_id)  # 有多少 replying tweets 被 retweeted？
sum(reply_data$tgt %in% retweet_map$tweet_id)  # 有多少 replied tweets 被 retweeted？


```

```{r}
co_reply_edges_v2 <- read_csv("data/mumin_csv/reply_reply_to_tweet.csv") |>
  filter(tgt %in% misinfo_tweet_id) |>  # 被 reply 的 tweet 是 misinformation
  rename(replied_tweet_id = tgt) |> 
  # 找出这个 misinformation tweet 被谁 retweeted 过
  left_join(retweet_map, by = c("replied_tweet_id" = "tweet_id")) |>
  rename(user_replied = retweeter_id) |> 
  filter(!is.na(user_replied)) |> 
  group_by(replied_tweet_id) |> 
  summarise(
    pairs = list(as.data.frame(t(combn(user_replied, 2)))), 
    .groups = "drop"
  ) |> 
  unnest(pairs) |> 
  mutate(
    user_1 = pmin(V1, V2),
    user_2 = pmax(V1, V2)
  ) |> 
  select(user_1, user_2) |> 
  group_by(user_1, user_2) |> 
  summarise(weight = n(), .groups = "drop") |> 
  mutate(type = "co-reply-v2")

table(co_reply_edges_v2$weight)
unique(co_reply_edges_v2$user_1)
unique(co_reply_edges_v2$user_2)

setdiff(co_reply_edges_v2$user_2,covid_wuhan_misinfo$retweeter_id)
setdiff(co_reply_edges_v2$user_1,covid_wuhan_misinfo$retweeter_id)
```


```{r}
# Follow
# 获取 retweeter 列表
retweeter_id_misinfo <- unique(covid_wuhan_misinfo$retweeter_id)

# 构造 Follow 边
retweeter_follow_edges <- read_csv("data/mumin_csv/user_follows_user.csv") |>
  rename(user_a = src, user_b = tgt) |> 
  # 保留两者都在 retweeter 列表中的记录
  filter(user_a %in% retweeter_id_misinfo & user_b %in% retweeter_id_misinfo) |> 
  mutate(
    user_1 = pmin(user_a, user_b),  # 保持小的 ID 在前，去掉方向性
    user_2 = pmax(user_a, user_b)
  ) |> 
  select(user_1, user_2) |> 
  distinct() |>  # 去重，防止重复记录 A -> B 和 B -> A
  mutate(weight = 1, type = "follow")

retweeter_follow_edges

```

```{r}

retweeter_follow_edges_directed <- read_csv("data/mumin_csv/user_follows_user.csv") |> 
  rename(follower = src, followee = tgt) |> 
  # 只保留 retweeter 之间的关注关系（有方向）
  filter(follower %in% retweeter_id_misinfo & followee %in% retweeter_id_misinfo) |> 
  mutate(
    weight = 1,          # 默认权重为 1
    type = "follow_directed"      # 明确说明是关注关系
  ) |> 
  select(follower, followee, weight, type)

```


```{r}
follow_data<-read_csv("data/mumin_csv/user_follows_user.csv") 
sum(follow_data$src %in% covid_wuhan_misinfo$retweeter_id)
sum(follow_data$tgt %in% covid_wuhan_misinfo$retweeter_id)

sum(covid_wuhan_misinfo$retweeter_id%in% follow_data$src)
sum(covid_wuhan_misinfo$retweeter_id%in% follow_data$tgt)
```


```{r}
# 合并所有无向边
edges_undirected <- bind_rows(
  co_retweet_edges, 
  co_quote_edges, 
  co_reply_edges, 
  co_reply_edges_v2, 
  retweeter_follow_edges
) |> 
  distinct()

# 合并所有有向边（如果需要方向性的分析）
edges_directed <- retweeter_follow_edges_directed |> 
  distinct()

write_csv(edges_undirected, "data/edges_undirected.csv")
write_csv(edges_directed, "data/edges_directed.csv")


```



# Analysis

```{r}
edges_undirected <- read_csv("data/edges_undirected.csv")


edges_undirected <- edges_undirected |> 
  mutate(user_1 = as.character(user_1),
         user_2 = as.character(user_2))

edges_clean <- edges_undirected |> 
  filter(user_1 != user_2) |> 
  group_by(user_1, user_2) |> 
  summarise(
    weight = sum(weight),  # 合并权重
    type = paste(unique(type), collapse = ";"),  # 合并类型标签
    .groups = "drop"
  )


net <- network(edges_clean[, c("user_1", "user_2")], directed = FALSE)

# 添加权重属性
set.edge.attribute(net, "co-retweet", str_detect(edges_clean$type, "co-retweet"))
set.edge.attribute(net, "co-quote", str_detect(edges_clean$type, "co-quote"))
set.edge.attribute(net, "co-reply-v2", str_detect(edges_clean$type, "co-reply-v2"))

set.edge.attribute(net, "weight", edges_clean$weight)


# 逐个作图
plot.network(
  net,
  displaylabels = FALSE,
  vertex.cex = 0.2,
  vertex.col = "orange",
  edge.lwd = 0.5,  # 调小边的宽度
  edge.col = ifelse(get.edge.attribute(net, "co-retweet"), 
                    adjustcolor("red", alpha.f = 0.2),  # 更透明的红色
                    "gray95"),  # 不关注的边几乎看不到
  main = "Network with co-retweet Edges"
)

# 2. 换成 co-quote 的边
plot.network(
  net,
  displaylabels = FALSE,
  vertex.cex = 0.2,
  vertex.col = "orange",
  edge.lwd = 1,
  edge.col = ifelse(get.edge.attribute(net, "co-quote"), 
                    adjustcolor("blue", alpha.f = 0.3), 
                    "gray95"),
  main = "Network with co-quote Edges"
)


# 3. 再换 co-reply-v2
plot.network(
  net,
  displaylabels = FALSE,
  vertex.cex = 0.2,
  vertex.col = "orange",
  edge.lwd = 1,
  edge.col = ifelse(get.edge.attribute(net, "co-reply-v2"), 
                    adjustcolor("green", alpha.f = 0.3), 
                    "gray95"),
  main = "Network with co-reply-v2 Edges"
)

plot.network(
  net,
  displaylabels = FALSE,
  vertex.cex = 0.1,  # 调小节点
  vertex.col = "orange",
  edge.lwd = 0.5,  # 调细边
  edge.col = "gray80",
  edge.transparency = 0.5,  # 增加透明度（statnet中用 adjustcolor() 处理颜色）
  main = "Network with co-retweet Edges"
)


```

# Reference


# Appendix

Take a glimpse of data:

```{r}
# 设置数据路径（请替换为你自己电脑上的路径）
data_path <- "data/mumin_csv/"

# 定义要读取的文件列表
files <- c(
  "claim.csv", "user.csv", "tweet.csv", "reply.csv", "article.csv",
  "tweet_discusses_claim.csv", "article_discusses_claim.csv",
  "user_follows_user.csv", "user_retweeted_tweet.csv",
  "reply_quote_of_tweet.csv", "reply_reply_to_tweet.csv"
)

# 用lapply预览前几行（避免爆内存）
previews <- lapply(files, function(fname) {
  fpath <- file.path(data_path, fname)
  tryCatch(
    read_csv(fpath, n_max = 5),
    error = function(e) tibble::tibble(error = paste("Error:", e$message))
  )
})

names(previews) <- files

previews$`claim.csv`

previews$`user.csv`                  
previews$`tweet.csv`
previews$`reply.csv`                  
previews$`article.csv`
previews$`tweet_discusses_claim.csv`  
previews$`article_discusses_claim.csv` 
previews$`user_follows_user.csv`      
previews$`user_retweeted_tweet.csv`
previews$`reply_quote_of_tweet.csv`   
previews$`reply_reply_to_tweet.csv` 

```

```{=html}
<script>
document.addEventListener("DOMContentLoaded", function() {
    const toc = document.getElementById("TOC");
    if (toc) {
        const sourceLink = document.createElement("div");
        sourceLink.innerHTML = `
            <div class="toc-source">
                <a href="https://github.com/troy-yu-cheng/sna-final-paper" 
                   target="_blank" 
                   class="github-button">
                   <svg xmlns="http://www.w3.org/2000/svg" 
                        viewBox="0 0 24 24" 
                        width="16" 
                        height="16" 
                        fill="currentColor"
                        style="vertical-align: middle; margin-right: 5px;">
                     <path d="M12 0C5.373 0 0 5.373 0 12c0 5.303 3.438 9.8 8.207 11.387.6.113.82-.26.82-.577v-2.157c-3.338.726-4.033-1.416-4.033-1.416-.546-1.386-1.332-1.756-1.332-1.756-1.09-.745.083-.73.083-.73 1.205.084 1.84 1.237 1.84 1.237 1.07 1.832 2.807 1.303 3.492.996.108-.774.418-1.303.76-1.602-2.665-.3-5.466-1.332-5.466-5.93 0-1.311.468-2.382 1.237-3.222-.124-.302-.536-1.52.118-3.163 0 0 1.008-.322 3.3 1.23a11.516 11.516 0 0 1 3.002-.403 11.486 11.486 0 0 1 3.002.403c2.292-1.552 3.3-1.23 3.3-1.23.654 1.644.242 2.861.118 3.163.77.84 1.236 1.911 1.236 3.222 0 4.61-2.807 5.627-5.48 5.922.43.372.812 1.103.812 2.222v3.293c0 .321.218.694.825.576C20.565 21.796 24 17.3 24 12 24 5.373 18.627 0 12 0z"/>
                   </svg>
                   View source
                </a>
            </div>
        `;
        toc.appendChild(sourceLink);
    }
});
</script>
```

